[
  {
    "objectID": "00-how-to-use-this-site.html",
    "href": "00-how-to-use-this-site.html",
    "title": "How to use this site",
    "section": "",
    "text": "There will be several topics on the sidebar, choose one then copy, learn, find errors and improve."
  },
  {
    "objectID": "aws-iam-auth.html",
    "href": "aws-iam-auth.html",
    "title": "What can we use to authenticate to AWS?",
    "section": "",
    "text": "IAM Access Keys _It is composed of:\n\nAn access key ID\nA secret access key _We use it for programmatic access to AWS. For example if we are using the command line interface to launch a service on AWS we would need to have an access key to be able to authenticate. _We use it for the API and when accessing AWS via the SDK _We can add MFA protection to the API calls. _We put these keys in program code or at a command prompt when using the AWS CLI or the AWS PowerShell tools. IMPORTANT: We can create, modify, view or rotate access keys.\n\nIAM Console Password\n\nUser account will have a password.\nWe use a username and a password to access the AWS Management Console (Visual Interface in which we can interact with AWS)\nThis can be also protected with MFA.\n\nIAM Server Certificate / Signing Certificate\n\nSecure Sockets Layer (SSL) / Transport Layer Security (TLS) certificates that we can use to authenticate with some AWS services\nWe can provision, manage and deploy this authentication using the AWS Certificate Manager (ACM)."
  },
  {
    "objectID": "aws-iam-billing-alarm.html",
    "href": "aws-iam-billing-alarm.html",
    "title": "Setting Billing Alarm",
    "section": "",
    "text": "Email alert to let us know when we reach certain dollars threshold.\nOnly the root account has access to modify billing information\n\nBilling dashboard\nBilling Preferences\n\nReceive Free Tier Usage Alerts (Enter an e-mail)\nReceive Billing Alerts\nManage Billing Alerts\nIn CloudWatch go to Billing\nCreate Alarm\nConfiguration, then Next\nCreate a New Topic\nSet an e-mail endpoint\nSet a name\nCreate Alarm\nView SNS subscriptions\nGo to your e-mail and confirm the SNS"
  },
  {
    "objectID": "aws-iam-creating-users.html",
    "href": "aws-iam-creating-users.html",
    "title": "Create IAM Users",
    "section": "",
    "text": "Create a user account that we can log on.\n\nWithin IAM go to Users\nAdd user\nGive it a friendly name\nSelect AWS access type\n\nProgrammatic access (an access key will be created)\nAWS Management Console access\n\nSet auto/custom password\nTick Require password reset if needed\n\n\nSet permissions\n\nWe can:\n\nAdd the user to a group (create a group if we haven’t created any groups yet)\nCopy permissions from existing user\nAttach existing policies directly\n\n\n\n5.1 We created a group a gave it AdministratorAccess\n\nCreate user\nWe get an Access key ID and a Secret access key which show only once\nClose"
  },
  {
    "objectID": "aws-iam-pass-policy.html",
    "href": "aws-iam-pass-policy.html",
    "title": "Setting Password Policy",
    "section": "",
    "text": "A password policy is a set of rules that define the type of password an IAM user can set.\n\nGo to Account Settings\nSpecify the password policy."
  },
  {
    "objectID": "aws-iam-sign-in-link.html",
    "href": "aws-iam-sign-in-link.html",
    "title": "Sign-in Link",
    "section": "",
    "text": "_This is the URL that users will use to log into the AWS account _We can modify it to be more user-friendly _The custom-value we choose does need to be unique\nhttps://custom-value.signin.aws.amazon.com/console\nThe custom-value is the alias for the account name, so any user account can be sign-in using this alias."
  },
  {
    "objectID": "aws-iam-sts.html",
    "href": "aws-iam-sts.html",
    "title": "Security Token Service (STS)",
    "section": "",
    "text": "A user may need some information that is only available at another account. How does this user access that information?\nUser ————> Role ————-> AWS STS ——–> Authentication ——–> User ————> Information\n_AWS STS is a web service that enables you to request temporary, limited-privilege credentials for IAM users or users that we authenticate (federated users) _In other words, users can have temporary, limited-privilege credentials for accessing certain services. _AWS STS is a global service, and they go to a single endpoint https://sts.amazonaws.com _By default, all regions are enabled for STS but can be disabled."
  },
  {
    "objectID": "aws-iam.html",
    "href": "aws-iam.html",
    "title": "Identity and Access Management (IAM)",
    "section": "",
    "text": "IAM is the service that provides USERS, GROUPS and POLICIES so that we can access our AWS accounts and we can do that securely."
  },
  {
    "objectID": "aws-iam.html#iam-users",
    "href": "aws-iam.html#iam-users",
    "title": "Identity and Access Management (IAM)",
    "section": "IAM Users",
    "text": "IAM Users\n\nIt is an entity that represents a person or service (a user account that can be used by a service rather than human being)\nWe can apply a policy which is a list of permissions which says what this user is allowed to do on AWS\nBy default, users cannot do anything in the account. Users need a policy in order to do something in the account.\nUp to 5000 users per AWS account.\nEach user account will have a friendly name and an Amazon Resource Name (ARN) which uniquely identifies the user across AWS.\n\n\nA user account can be assigned:\n\nAccess key ID\nSecret access key _This is called a “keeper” and is a way a user account can programmatically access the AWS API, CLI, SDK and other development tools.\nA password for access to the AWS Management Console.\nEach user has a friendly name and an Amazon Resource Name (ARN) which uniquely identifies the user across AWS.\n\nRoot User Credentials:\n\nemail address that we used to create the account\npassword _A Root account has full administrative permissions and cannot be restricted. Do not user root user credentials. _BEST PRACTICE: Create a user account and associate a policy or administrative permissions.\n\nService Accounts:\n\nUsers accounts that are created to represent an application."
  },
  {
    "objectID": "aws-iam.html#iam-groups",
    "href": "aws-iam.html#iam-groups",
    "title": "Identity and Access Management (IAM)",
    "section": "IAM Groups",
    "text": "IAM Groups\n_It is a collection of users _We can also apply a policy that is relevant to the user accounts that we have put into a particular group. These policies are attached to the users within the group. _A group is not an identity and cannot be identified as a principal in an IAM policy. Essentially, it means that a group is not something that you can write into a permissions policy but we can take a permission policy and apply it to a group which is then passed to users within the group. _We can nest groups so we cannot have groups within groups. _BEST PRACTICE: Use groups to assign permissions to users _BEST PRACTIVE: Use the principal of least privilege when assigning permissions."
  },
  {
    "objectID": "aws-iam.html#iam-roles",
    "href": "aws-iam.html#iam-roles",
    "title": "Identity and Access Management (IAM)",
    "section": "IAM Roles",
    "text": "IAM Roles\n_Roles are created and then “assumed” by a trusted entity. _We use it for delegation _We can also apply a policy _Roles are esentially used by services _It defines a set of permissions for making service requests in AWS\n\nFor example:\n\nWe can have a role that provides access to the S3 storage system or DynamoDB database service or AWS Lambda.\nThis role is provided those permissions to execute the specific requests.\n\n\n_We assign that role to another AWS service and that service is then able to start Lambda and make it work. _THEREFORE: We use IAM Roles for delegate permissions to resources for users and services without using permanent credentials (e.g. username and password). IAM users or AWS Services can assume a role to obtain temporary security credentials that can be used to make AWS API calls."
  },
  {
    "objectID": "aws-iam.html#iam-policy",
    "href": "aws-iam.html#iam-policy",
    "title": "Identity and Access Management (IAM)",
    "section": "IAM Policy",
    "text": "IAM Policy\n_Documents that define permissions that can be applied to users, groups and roles _Written in JSON _All permissions are implicitly denied by default _The most restrictive policy is always applied _The IAM Policy Simulator is a tool that helps us understand, test and validate the effects of access control policies _There is a Conditional Element that we can used to apply conditional logic (the permission is applied only if x argument satisfies y value)."
  },
  {
    "objectID": "aws-infra-vpc.html",
    "href": "aws-infra-vpc.html",
    "title": "Virtual Private Cloud (VPC)",
    "section": "",
    "text": "VPC _A VPC is a logically isolated section of the AWS cloud within a region where we can launch our own resources. _Within a VPC we can create our own networks using our own IP ranges. _A VPC sits within a region and we can have up to 5 VPCs within each region by default. _By default, one VPC is create by default in every one of those regions around the world. _Every VPC has a Classless Inter Domain Routing (CIDR) block.\nSubnets _Within a VPC we create public or private subnets which sits within AZs. _Then we launch our resources into the subnets. _Auto-assign public IPv4 will allow EC2 instances pick an IPv4 address automatically. _Each subnet will have a different block of addresses but they will be within the overall CIDR block of the VPC.\nVPC Router _The way we communicate between AZs and within subnets, whether they are in the same AZ or not, is through the VPC Router _Every VPC has a Route Table which has a IP address range which helps in the communication between all AZs (CIDR). _By defining the IP Range for the VPC, the route table will automatically be configured to route between all the AZs and subnets within the VPC\nInternet Gateway _It allows us to access the outside world. For example, send requests out to an Internet website through the IG _We need to specify the Internet Gateway ID and an IP address as a destination\nIncident Gateway _It allows us to be able to communicate outside of AWS to the Internet"
  },
  {
    "objectID": "aws-infra.html",
    "href": "aws-infra.html",
    "title": "Global Infrastructure",
    "section": "",
    "text": "Region _It is a geographical area with 2 or more AZs, isolated from other AWS Regions.\nAvailability Zone (AZ) _One or more data centers that are physically separate and isolated from other AZs _They are connected with low latency, highly available, high bandwidth links _We have networking connectivity between all of the AZs as well\n\nTherefore, we can deploy our resources within a region but into separate AZs for high availability and redudancy. If there is an outage on one AZ, the resources on the other one are not affected.\n\nEdge Location (CloudFront) _A location with a cache of content that can be delivered at low latency to users _The idea is to reduce latency when accessing cached content\nRegional Edge Cache (CloudFront) _Larger caches that sit between AWS Services and Edge Locations\n\nCloudFront is all about caching content.\n\nGlobal Network (Private) _Highly available, low-latency private global network interconnecting every data center, AZ, and AWS Region"
  },
  {
    "objectID": "deng-data-cleaning.html",
    "href": "deng-data-cleaning.html",
    "title": "Cleaning Data",
    "section": "",
    "text": "Data cleaning is the process of polishing raw data to keep the entries consistent.\n\nFilling up empty fields with default values\nRemoving characters that are not alpha-numeric\nRemoving stop words\nRemoving HTML tags\n\nThe process also focuses on retaining relevant information from the collected data. Drop unnecessary components of the collected information.\nIt is important to separate whether information were missing in the original source and not missed out due to errors throughout the collection process\n\nimport pandas as pd\n\n\npdf = pd.read_csv('.csv')\npdf.some_column.fillna(value=\"na\", inplace=True)"
  },
  {
    "objectID": "deng-data-collection-html.html",
    "href": "deng-data-collection-html.html",
    "title": "Collecting HTML Data",
    "section": "",
    "text": "Collect HTML and save it in CSV format\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nresponse = requests.get(url='https://developer.mozilla.org/')\n\n\nhtml_soup = BeautifulSoup(response.text, 'html.parser')\n\n\n# Find all <a> tags with href in it, returns a list\nhtml_soup.find_all('a', href=True)[:2]\n\n[<a href=\"#content\" id=\"skip-main\">Skip to main content</a>,\n <a href=\"#top-nav-search-input\" id=\"skip-search\">Skip to search</a>]\n\n\n\n# Convert a PageElement to a string and perform string operations\nstr(a).find(\"something\")\n\n-1\n\n\n\n# Select to extract the text inside a div tag with a specific id\nlen(html_soup.select('div#root'))\n\n1\n\n\n\n# Find all div tags of a specific class\nhtml_soup.find_all('div', class_=\"submenu-icon\")\n\n[<div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon html\"></div>,\n <div class=\"submenu-icon css\"></div>,\n <div class=\"submenu-icon javascript\"></div>,\n <div class=\"submenu-icon http\"></div>,\n <div class=\"submenu-icon apis\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon learn\"></div>,\n <div class=\"submenu-icon learn\"></div>,\n <div class=\"submenu-icon html\"></div>,\n <div class=\"submenu-icon css\"></div>,\n <div class=\"submenu-icon javascript\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon\"></div>]\n\n\n\n# Images with src attributes\nhtml_soup.select(\"img\")[0][\"src\"]\n\n'/assets/mdn_contributor.png'"
  },
  {
    "objectID": "deng-data-collection-json.html",
    "href": "deng-data-collection-json.html",
    "title": "Collecting JSON Data",
    "section": "",
    "text": "JSON stores data as key-value and/or key-array fields. JSON is considered interchangeable or program independent since most programming languages support key-value data structures.\nDuring data collection process, the information is extracted as-is so that it can be kept in the original form.\n\nimport json\nimport urllib.request as request\n\n\nresponse = request.urlopen('https://www.reddit.com/r/all.json')\n\n\nsource = response.read()\n\n\njson_data = json.loads(source)\n\n\njson_data['data']['children'][0]['data']['media_embed']\n\n{}"
  },
  {
    "objectID": "deng.html",
    "href": "deng.html",
    "title": "Why Data Engineering is so important?",
    "section": "",
    "text": "The process of data engineering can be broken down into a few parts:\n\nData Collection\nData Cleaning\nData Preprocessing"
  },
  {
    "objectID": "mlplatforms-databricks-arquitecture.html",
    "href": "mlplatforms-databricks-arquitecture.html",
    "title": "Arquitecture and Performance",
    "section": "",
    "text": "While data lakes are a great solution for holding large quantities of raw data, they lack important features for data reliability and quality. Also, data lakes do not offer as good of performance as that of data warehouses.\nProblems with data lakes related to data reliability - Lack of ACID transaction support. It makes it impossible to mix updates, appends and reads. - Lack of schema enforcement. It creates inconsistent and low quality data. - Lack of integration with a data catalog. It does not create a single source of truth.\nProblems with data lakes related to performance. Data is mostly kept in immutable files leading to - Ineffective partitioning. Partitioning tends to be ineffective if the wrong field was selected for partitioning or due to high cardenality columns. - Too many small files. Appending new data takes the shape of simply adding new files, usually small files.\n\n\n\n\nDelta Lake is a file-based open source storage format. It offers: - ACID transaction guarantees. No partial or corrupted files. - Scalable data and metadata handling. Spark scales out all the metadata processing. - Audit history and time travel. It provides a transaction log with details about every change to data which allows us to revert to earlier versions for rollback or to reproduce experiments. - Schema enforcement and schema evolution. - Unified streaming and batch data processing\nIt is compatible with Apache Spark, uses Delta Tables which are based on Apache Parquet. Delta Lake has a transaction log which converts it as the single source of truth. It makes it possible multi-user work.\n\n\n\nThe execution engine has to provide the same performance as a data warehouse, while still having the scalability of a data lake. Photon provides this. It is the next generation query engine.\nPhoton is compatible with Spark APIs, implementing a more general execution framework for efficient processing of data with support of the Spark APIs.\nWith Photon we see: - increased speed for use cases such as data ingestion, ETL, streaming, data science, and interactive queries"
  },
  {
    "objectID": "mlplatforms-databricks-arquitecture.html#unified-governance-and-security",
    "href": "mlplatforms-databricks-arquitecture.html#unified-governance-and-security",
    "title": "Arquitecture and Performance",
    "section": "Unified Governance and Security",
    "text": "Unified Governance and Security"
  },
  {
    "objectID": "mlplatforms-databricks.html",
    "href": "mlplatforms-databricks.html",
    "title": "Data Lakehouse",
    "section": "",
    "text": "A data warehouse was designed to collect and consolidate the influx of data which was being generated and collected at high volumes and at a fast pace.\n\nMain advantages\n\ndata is in a production-ready state for business intelligence and analytics\ndata is structured and cleaned\nschemas are predefined\n\nMain disadvantages\n\nthey were not designed for semi-structured or unstructured data\nthey becomes very expensive if we try to store data that do not fit the schema (inflexible schemas)\nthey struggle with the volume, velocity, variety of big data and they usually take long processing time"
  },
  {
    "objectID": "mlplatforms-databricks.html#data-lakes",
    "href": "mlplatforms-databricks.html#data-lakes",
    "title": "Data Lakehouse",
    "section": "Data Lakes",
    "text": "Data Lakes\nIn a data lake, structured, semi-structured and unstructured data can live simultaneously, collected in the volumes and speed necessary.\n\nMain advantages\n\nFlexible data storages, multiple data types can be stored in it\nCost efficiency in the cloud allows data streaming\nmain disadvantages of data warehouses are solved\n\nMain disadvantages\n\nNot support for transactional data\nDo not enforce data quality, so data reliability is questionable since there are several formats\nThen, data governance is a challenge as well as data privacy enforcement"
  },
  {
    "objectID": "mlplatforms-databricks.html#data-lakehouses",
    "href": "mlplatforms-databricks.html#data-lakehouses",
    "title": "Data Lakehouse",
    "section": "Data Lakehouses",
    "text": "Data Lakehouses\n\nAll use cases (predictive modeling, SQL, BI, streaming, etc.) within a single platform\nOne security and governance approach for all data assets on all clouds\nOpen and reliable arquitecture which combines the benefits of a data warehouse and a data lake to efficiently handle all data types\nIt offers decoupled storage from compute, each operates on their own clusters, allowing them to scale independently to support specific needs\n\n\nDatabricks Lakehouse Platform\nFounded in 2013 by the creators of Apache Spark, Delta Lake and MLFlow.\n\nReliability and performance of Delta Lake\nGovernance for data and AI with Unity Catalog\nPersona-based uses cases for all data team members\nProvides instant and serverless compute, the compute layer is provided and managed by Databricks"
  },
  {
    "objectID": "production-ready-deploy-maintenance.html",
    "href": "production-ready-deploy-maintenance.html",
    "title": "Deployment and Maintenance",
    "section": "",
    "text": "The deployment setting are usually different from the development settings. Therefore, different sets of tools are often involved when moving a FFP to production.\nDeployment may introduce problems that weren’t visible during development such as limited computational resources.\n\nUse additional time to improve user experience.\n\nThe quality of data and model performance needs to be monitored consistently to provide stable services to targeted users."
  },
  {
    "objectID": "production-ready-evaluation.html",
    "href": "production-ready-evaluation.html",
    "title": "Project Evaluation",
    "section": "",
    "text": "The team should revisit the discussions made during project planning to\n\nevaluate whether the project has been carried out successfully or not\n\nDetails of the project need to be recorded to\n\nsuggest and discuss potential improvements so that next projects can be achieved more efficiently."
  },
  {
    "objectID": "production-ready-ffp.html",
    "href": "production-ready-ffp.html",
    "title": "Build Fully Featured Products",
    "section": "",
    "text": "Once the feasibility of the project has been confirmed by the MVP, it must be packaged into an FFP.\n\nPolish up the MVP to build a production-ready deliverable with various optimizations\n\nIntroduce additional data preparation techniques to improve the quality of input data\nSlightly augment the model pipeline for greater model performance if necessary\n\nReimplement the whole pipeline (data preparation and model training) on web services for higher throughput and quality.\n\nAWS, for example."
  },
  {
    "objectID": "production-ready-mvp.html",
    "href": "production-ready-mvp.html",
    "title": "Build Minimum Viable Products",
    "section": "",
    "text": "Assert directions is clear for everyone, then build a MVP.\n\nBuild a simplistic version of the deliverable that showcases the project’s value.\nUnderstand the project’s difficulties and reject paths with greater risks or less promising outcomes\n\nUse the fail fast, fail often philosophy\n\nWork with partially sampled datasets in development settings and ignore insignificant optimizations."
  },
  {
    "objectID": "production-ready-planning.html",
    "href": "production-ready-planning.html",
    "title": "Effective Project Planning",
    "section": "",
    "text": "The project lead must\n\nclearly define what is the purpose of the project (new product, improve existing service, save operational cost)\nclearly define what needs to be achieved by the project\nunderstand groups that can affect or be affected by the project (stakeholders).\n\nThe evaluations metrics need to be defined and agreed upon as they will be revisited during project evaluation.\n\nBusiness-related metrics\nModel-based metrics\nDue dates\nResource usage (available resources)\nTradeoffs can be made between the various metrics. Slight decrease in accuracy may be acceptable if meeting latency requirements is more critical to the project.\n\nTeam members group together to discuss\n\nhow to achieve business objectives using available resources.\nkey members should have a thorough understanding of the available resources that can be allocated to the project.\nindividual responsabilities as well as stakeholders responsabilities\n\nDefine milestones and their associated tasks\n\nA milestone refers to a point in a project where a significant event occurs\nThe ordering of tasks that lead to the goal is called the critical path\nAvailable resources need to be allocated to each task\nset aside a portion of the resources as a backup\n\nEstablish the timeline, an estimate of how long the project would be.\nGenerate a well-documented project playbook, a thorough description of how the project will be carried out, evaluated and defines business objectives. Refer to PMBOK or PRINCE2.\n\nOverview of key deliverables\nList of stakeholders\nGantt chart defining steps and bottlenecks\nDefinition of responsabilities\nTimelines\nEvaluation criteria\n\nOnce the playbook is constructed, every stakeholder must review and revise it until everyone agrees with the contents.\nDefine the process that the team will follow to update other team members and ensure on-time delivery of the project.\n\nSelect a project management methodologies"
  },
  {
    "objectID": "production-ready.html",
    "href": "production-ready.html",
    "title": "Less Theory More Application",
    "section": "",
    "text": "Construct and deploy complex models in machine learning frameworks.\nThe real difficulty with machine learning projects is not only selecting the right algorithm for the given problem but also efficiently preprocessing the necessary data in the right format and providing a stable service.\nThe idea of this “Production-Ready” section is to walk through every step of a machine learning project\nFollowing the Production-Ready book\nEvery machine learning project should begin with planning and understanding the difficulty of the given problem.\nWhat is the scope of the project?\nOnce this question is answered, the next step is to build a MVP.\nIn the context of machine learning, the process of creating an MVP involves: - Preparing a dataset - Exploring various model architectures to come up with a working solution to the given problem\nWhat is machine learning? How does it shapes our daily lives?\nHow are machine learning projects typically carried out? How are they structured?\nProject planning - Comprehension of business objectives - Define appropriate evaluation metrics - Identification of stakeholders - Resource planning - Differences between a minimum viable product (MVP) and a fully featured product (FFP)\nAim: Construct a machine learning project playbook that clearly describes - The goal of the project - Milestones - Tasks - Resource allocation - Timeline"
  },
  {
    "objectID": "production-ready.html#what-is-machine-learning",
    "href": "production-ready.html#what-is-machine-learning",
    "title": "Less Theory More Application",
    "section": "What is machine learning?",
    "text": "What is machine learning?\nIt is a set of methods within the field of artificial intelligence. By extracting meaningful patterns from historical data, the goal of machine learning is to build a model that makes sensible predictions and decisions for newly collected data.\nWe can capture a given pattern with different techniques: deep learning which exploits artificial neural networks, decision trees, bagging or boosting ensembles, linear models, etc.\nOne of the advantages of deep learning over traditional machine learning techniques is that ANNs enable the automatic selection of necessary features whereas traditional ML techniques require features to be manually selected. ANNs are made up of components called neurons. A group of neurons forms a layer and multiple layers are linked together to form a network. We can understood this arquitecture as multiple steps of nested instructions, thus, as the input data passes through the network, each neuron extracts different information and the model is trained to select the most relevant features for the given task. If we consider neurons as the building blocks for pattern recognition then the usual believe is that deeper networks lead to greater performance as they learn the details better.\nThis advantage allows deep learning methods to adapt to a broader range of problems due to its high flexibility. But it is not free, we need large and diverse enough training dataset to train a DL model properly. This leads an increase in training time."
  },
  {
    "objectID": "production-ready.html#overview-of-machine-learning-projects",
    "href": "production-ready.html#overview-of-machine-learning-projects",
    "title": "Less Theory More Application",
    "section": "Overview of Machine Learning Projects",
    "text": "Overview of Machine Learning Projects\nIn general, machine learning projects can be split into the following phases:\n\nProject planning\nBuilding MVPs\nBuilding FFPs\nDeployment and maintenance\nProject evaluation"
  },
  {
    "objectID": "venv-conda.html",
    "href": "venv-conda.html",
    "title": "Setting up virtual environments with conda",
    "section": "",
    "text": "For more information see Managing environments.\nCreate a virtual environment (replace venvname with the name of environment)\nfoo@bar:~$ conda create -n venvname python=3.9\nVerify that the new environment was installed correctly\nfoo@bar:~$ conda info --envs\nfoo@bar:~$ conda env list\nActivate or deactivate the environment\nfoo@bar:~$ conda activate venvname\nfoo@bar:~$ conda deactivate\nActivate the environment and install required packages\nfoo@bar:~$ pip install -r requirements.txt\nfoo@bar:~$ conda install pytorch torchvision torchaudio cudatoolkit=11.2 -c pytorch -c nvidia\nfoo@bar:~$ conda install --force-reinstall -y -q --name py39 -c conda-forge --file requirements.txt\nSave the environment to a file\nfoo@bar:~$ conda env export > venv.yml\nCreate a environment from a YAML file\nfoo@bar:~$ conda env create -f venv.yml\nDelete the environment\nfoo@bar:~$ conda remove -n venvname --all"
  }
]