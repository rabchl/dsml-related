[
  {
    "objectID": "00-how-to-use-this-site.html",
    "href": "00-how-to-use-this-site.html",
    "title": "How to use this site",
    "section": "",
    "text": "There will be several topics on the sidebar, choose one then copy, learn, find errors and improve."
  },
  {
    "objectID": "aws-iam-auth.html",
    "href": "aws-iam-auth.html",
    "title": "What can we use to authenticate to AWS?",
    "section": "",
    "text": "IAM Access Keys _It is composed of:\n\nAn access key ID\nA secret access key _We use it for programmatic access to AWS. For example if we are using the command line interface to launch a service on AWS we would need to have an access key to be able to authenticate. _We use it for the API and when accessing AWS via the SDK _We can add MFA protection to the API calls. _We put these keys in program code or at a command prompt when using the AWS CLI or the AWS PowerShell tools. IMPORTANT: We can create, modify, view or rotate access keys.\n\nIAM Console Password\n\nUser account will have a password.\nWe use a username and a password to access the AWS Management Console (Visual Interface in which we can interact with AWS)\nThis can be also protected with MFA.\n\nIAM Server Certificate / Signing Certificate\n\nSecure Sockets Layer (SSL) / Transport Layer Security (TLS) certificates that we can use to authenticate with some AWS services\nWe can provision, manage and deploy this authentication using the AWS Certificate Manager (ACM)."
  },
  {
    "objectID": "aws-iam-billing-alarm.html",
    "href": "aws-iam-billing-alarm.html",
    "title": "Setting Billing Alarm",
    "section": "",
    "text": "Email alert to let us know when we reach certain dollars threshold.\nOnly the root account has access to modify billing information\n\nBilling dashboard\nBilling Preferences\n\nReceive Free Tier Usage Alerts (Enter an e-mail)\nReceive Billing Alerts\nManage Billing Alerts\nIn CloudWatch go to Billing\nCreate Alarm\nConfiguration, then Next\nCreate a New Topic\nSet an e-mail endpoint\nSet a name\nCreate Alarm\nView SNS subscriptions\nGo to your e-mail and confirm the SNS"
  },
  {
    "objectID": "aws-iam-creating-users.html",
    "href": "aws-iam-creating-users.html",
    "title": "Create IAM Users",
    "section": "",
    "text": "Create a user account that we can log on.\n\nWithin IAM go to Users\nAdd user\nGive it a friendly name\nSelect AWS access type\n\nProgrammatic access (an access key will be created)\nAWS Management Console access\n\nSet auto/custom password\nTick Require password reset if needed\n\n\nSet permissions\n\nWe can:\n\nAdd the user to a group (create a group if we haven’t created any groups yet)\nCopy permissions from existing user\nAttach existing policies directly\n\n\n\n5.1 We created a group a gave it AdministratorAccess\n\nCreate user\nWe get an Access key ID and a Secret access key which show only once\nClose"
  },
  {
    "objectID": "aws-iam-pass-policy.html",
    "href": "aws-iam-pass-policy.html",
    "title": "Setting Password Policy",
    "section": "",
    "text": "A password policy is a set of rules that define the type of password an IAM user can set.\n\nGo to Account Settings\nSpecify the password policy."
  },
  {
    "objectID": "aws-iam-sign-in-link.html",
    "href": "aws-iam-sign-in-link.html",
    "title": "Sign-in Link",
    "section": "",
    "text": "_This is the URL that users will use to log into the AWS account _We can modify it to be more user-friendly _The custom-value we choose does need to be unique\nhttps://custom-value.signin.aws.amazon.com/console\nThe custom-value is the alias for the account name, so any user account can be sign-in using this alias."
  },
  {
    "objectID": "aws-iam-sts.html",
    "href": "aws-iam-sts.html",
    "title": "Security Token Service (STS)",
    "section": "",
    "text": "A user may need some information that is only available at another account. How does this user access that information?\nUser ————> Role ————-> AWS STS ——–> Authentication ——–> User ————> Information\n_AWS STS is a web service that enables you to request temporary, limited-privilege credentials for IAM users or users that we authenticate (federated users) _In other words, users can have temporary, limited-privilege credentials for accessing certain services. _AWS STS is a global service, and they go to a single endpoint https://sts.amazonaws.com _By default, all regions are enabled for STS but can be disabled."
  },
  {
    "objectID": "aws-iam.html",
    "href": "aws-iam.html",
    "title": "Identity and Access Management (IAM)",
    "section": "",
    "text": "IAM is the service that provides USERS, GROUPS and POLICIES so that we can access our AWS accounts and we can do that securely."
  },
  {
    "objectID": "aws-iam.html#iam-users",
    "href": "aws-iam.html#iam-users",
    "title": "Identity and Access Management (IAM)",
    "section": "IAM Users",
    "text": "IAM Users\n\nIt is an entity that represents a person or service (a user account that can be used by a service rather than human being)\nWe can apply a policy which is a list of permissions which says what this user is allowed to do on AWS\nBy default, users cannot do anything in the account. Users need a policy in order to do something in the account.\nUp to 5000 users per AWS account.\nEach user account will have a friendly name and an Amazon Resource Name (ARN) which uniquely identifies the user across AWS.\n\n\nA user account can be assigned:\n\nAccess key ID\nSecret access key _This is called a “keeper” and is a way a user account can programmatically access the AWS API, CLI, SDK and other development tools.\nA password for access to the AWS Management Console.\nEach user has a friendly name and an Amazon Resource Name (ARN) which uniquely identifies the user across AWS.\n\nRoot User Credentials:\n\nemail address that we used to create the account\npassword _A Root account has full administrative permissions and cannot be restricted. Do not user root user credentials. _BEST PRACTICE: Create a user account and associate a policy or administrative permissions.\n\nService Accounts:\n\nUsers accounts that are created to represent an application."
  },
  {
    "objectID": "aws-iam.html#iam-groups",
    "href": "aws-iam.html#iam-groups",
    "title": "Identity and Access Management (IAM)",
    "section": "IAM Groups",
    "text": "IAM Groups\n_It is a collection of users _We can also apply a policy that is relevant to the user accounts that we have put into a particular group. These policies are attached to the users within the group. _A group is not an identity and cannot be identified as a principal in an IAM policy. Essentially, it means that a group is not something that you can write into a permissions policy but we can take a permission policy and apply it to a group which is then passed to users within the group. _We can nest groups so we cannot have groups within groups. _BEST PRACTICE: Use groups to assign permissions to users _BEST PRACTIVE: Use the principal of least privilege when assigning permissions."
  },
  {
    "objectID": "aws-iam.html#iam-roles",
    "href": "aws-iam.html#iam-roles",
    "title": "Identity and Access Management (IAM)",
    "section": "IAM Roles",
    "text": "IAM Roles\n_Roles are created and then “assumed” by a trusted entity. _We use it for delegation _We can also apply a policy _Roles are esentially used by services _It defines a set of permissions for making service requests in AWS\n\nFor example:\n\nWe can have a role that provides access to the S3 storage system or DynamoDB database service or AWS Lambda.\nThis role is provided those permissions to execute the specific requests.\n\n\n_We assign that role to another AWS service and that service is then able to start Lambda and make it work. _THEREFORE: We use IAM Roles for delegate permissions to resources for users and services without using permanent credentials (e.g. username and password). IAM users or AWS Services can assume a role to obtain temporary security credentials that can be used to make AWS API calls."
  },
  {
    "objectID": "aws-iam.html#iam-policy",
    "href": "aws-iam.html#iam-policy",
    "title": "Identity and Access Management (IAM)",
    "section": "IAM Policy",
    "text": "IAM Policy\n_Documents that define permissions that can be applied to users, groups and roles _Written in JSON _All permissions are implicitly denied by default _The most restrictive policy is always applied _The IAM Policy Simulator is a tool that helps us understand, test and validate the effects of access control policies _There is a Conditional Element that we can used to apply conditional logic (the permission is applied only if x argument satisfies y value)."
  },
  {
    "objectID": "aws-infra-vpc.html",
    "href": "aws-infra-vpc.html",
    "title": "Virtual Private Cloud (VPC)",
    "section": "",
    "text": "VPC _A VPC is a logically isolated section of the AWS cloud within a region where we can launch our own resources. _Within a VPC we can create our own networks using our own IP ranges. _A VPC sits within a region and we can have up to 5 VPCs within each region by default. _By default, one VPC is create by default in every one of those regions around the world. _Every VPC has a Classless Inter Domain Routing (CIDR) block.\nSubnets _Within a VPC we create public or private subnets which sits within AZs. _Then we launch our resources into the subnets. _Auto-assign public IPv4 will allow EC2 instances pick an IPv4 address automatically. _Each subnet will have a different block of addresses but they will be within the overall CIDR block of the VPC.\nVPC Router _The way we communicate between AZs and within subnets, whether they are in the same AZ or not, is through the VPC Router _Every VPC has a Route Table which has a IP address range which helps in the communication between all AZs (CIDR). _By defining the IP Range for the VPC, the route table will automatically be configured to route between all the AZs and subnets within the VPC\nInternet Gateway _It allows us to access the outside world. For example, send requests out to an Internet website through the IG _We need to specify the Internet Gateway ID and an IP address as a destination\nIncident Gateway _It allows us to be able to communicate outside of AWS to the Internet"
  },
  {
    "objectID": "aws-infra.html",
    "href": "aws-infra.html",
    "title": "Global Infrastructure",
    "section": "",
    "text": "Region _It is a geographical area with 2 or more AZs, isolated from other AWS Regions.\nAvailability Zone (AZ) _One or more data centers that are physically separate and isolated from other AZs _They are connected with low latency, highly available, high bandwidth links _We have networking connectivity between all of the AZs as well\n\nTherefore, we can deploy our resources within a region but into separate AZs for high availability and redudancy. If there is an outage on one AZ, the resources on the other one are not affected.\n\nEdge Location (CloudFront) _A location with a cache of content that can be delivered at low latency to users _The idea is to reduce latency when accessing cached content\nRegional Edge Cache (CloudFront) _Larger caches that sit between AWS Services and Edge Locations\n\nCloudFront is all about caching content.\n\nGlobal Network (Private) _Highly available, low-latency private global network interconnecting every data center, AZ, and AWS Region"
  },
  {
    "objectID": "deng-data-cleaning.html",
    "href": "deng-data-cleaning.html",
    "title": "Cleaning Data",
    "section": "",
    "text": "Data cleaning is the process of polishing raw data to keep the entries consistent.\n\nFilling up empty fields with default values\nRemoving characters that are not alpha-numeric\nRemoving stop words\nRemoving HTML tags\n\nThe process also focuses on retaining relevant information from the collected data. Drop unnecessary components of the collected information.\nIt is important to separate whether information were missing in the original source and not missed out due to errors throughout the collection process\n\nimport pandas as pd\n\n\npdf = pd.read_csv('.csv')\npdf.some_column.fillna(value=\"na\", inplace=True)"
  },
  {
    "objectID": "deng-data-collection-html.html",
    "href": "deng-data-collection-html.html",
    "title": "Collecting HTML Data",
    "section": "",
    "text": "Collect HTML and save it in CSV format\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\nresponse = requests.get(url='https://developer.mozilla.org/')\n\n\nhtml_soup = BeautifulSoup(response.text, 'html.parser')\n\n\n# Find all <a> tags with href in it, returns a list\nhtml_soup.find_all('a', href=True)[:2]\n\n[<a href=\"#content\" id=\"skip-main\">Skip to main content</a>,\n <a href=\"#top-nav-search-input\" id=\"skip-search\">Skip to search</a>]\n\n\n\n# Convert a PageElement to a string and perform string operations\nstr(a).find(\"something\")\n\n-1\n\n\n\n# Select to extract the text inside a div tag with a specific id\nlen(html_soup.select('div#root'))\n\n1\n\n\n\n# Find all div tags of a specific class\nhtml_soup.find_all('div', class_=\"submenu-icon\")\n\n[<div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon html\"></div>,\n <div class=\"submenu-icon css\"></div>,\n <div class=\"submenu-icon javascript\"></div>,\n <div class=\"submenu-icon http\"></div>,\n <div class=\"submenu-icon apis\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon learn\"></div>,\n <div class=\"submenu-icon learn\"></div>,\n <div class=\"submenu-icon html\"></div>,\n <div class=\"submenu-icon css\"></div>,\n <div class=\"submenu-icon javascript\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon\"></div>,\n <div class=\"submenu-icon\"></div>]\n\n\n\n# Images with src attributes\nhtml_soup.select(\"img\")[0][\"src\"]\n\n'/assets/mdn_contributor.png'"
  },
  {
    "objectID": "deng-data-collection-json.html",
    "href": "deng-data-collection-json.html",
    "title": "Collecting JSON Data",
    "section": "",
    "text": "JSON stores data as key-value and/or key-array fields. JSON is considered interchangeable or program independent since most programming languages support key-value data structures.\nDuring data collection process, the information is extracted as-is so that it can be kept in the original form.\n\nimport json\nimport urllib.request as request\n\n\nresponse = request.urlopen('https://www.reddit.com/r/all.json')\n\n\nsource = response.read()\n\n\njson_data = json.loads(source)\n\n\njson_data['data']['children'][0]['data']['media_embed']\n\n{}"
  },
  {
    "objectID": "deng.html",
    "href": "deng.html",
    "title": "Why Data Engineering is so important?",
    "section": "",
    "text": "The process of data engineering can be broken down into a few parts:\n\nData Collection\nData Cleaning\nData Preprocessing"
  },
  {
    "objectID": "mlplatforms-databricks-arquitecture.html",
    "href": "mlplatforms-databricks-arquitecture.html",
    "title": "Arquitecture",
    "section": "",
    "text": "Here we describe the main aspects of the arquitecture behind the Databricks Lakehouse Platform. We start by showing how Delta Lake solves many issues related to data reliability and how Photon improves the performance of DBLP. Then, we explain how DBLP solves many challenges of data governance and security with Unity Catalog, Delta Sharing, and the divided arquitecture into two planes: Control Plane and Data Plane."
  },
  {
    "objectID": "mlplatforms-databricks-arquitecture.html#data-reliability-and-performance",
    "href": "mlplatforms-databricks-arquitecture.html#data-reliability-and-performance",
    "title": "Arquitecture",
    "section": "Data Reliability and Performance",
    "text": "Data Reliability and Performance\nData needs to be reliable and clean since it’s the source for business insights. While data lakes are a great solution for holding large quantities of raw data, they lack important features for data reliability and quality (data swamps). Also, data lakes don’t often offer as good of performance as that of data warehouses.\nProblems with standard data lakes related to data reliability\n\nLack of Atomicity, Consistency, Isolation, Durability (ACID) transaction support. It makes it impossible to mix updates, appends and reads.\nLack of schema enforcement. It creates inconsistent and low quality data.\nLack of integration with a data catalog. It doesn’t create a single source of truth.\n\nProblems with data lakes related to performance\n\nData is mostly kept in immutable files.\nIneffective partitioning. Partitioning tends to be ineffective if the wrong field is selected for partitioning or due to high cardinality columns.\nToo many small files. Appending new data takes the shape of simply adding new files, usually small. It is a known root of query performance degradation.\n\n\nDelta Lake\nDelta Lake is a file-based open source storage format that runs on top of existing cloud data lakes. It is compatible with Apache Spark and uses Delta Tables which are based on Apache Parquet. Delta Tables provides versioning, reliability, metadata management, and time travel capabilities making all data types manageable.\nDelta Lake offers\n\nGuarantees for ACID transactions. No partial or corrupted files.\nScalable data and metadata handling. Spark scales out all the metadata processing.\nAudit history and time travel. It provides a transaction log with details about every change to data, which allows us to revert to earlier versions for rollback or to reproduce experiments. It makes it possible multi-user work and it creates a single source of truth.\nSchema enforcement and schema evolution.\nSupport for deletes, updates and merges allowing it to accommodate Change Data Capture (CDC), Slowly Changing Dimension (SCD) operations, and streaming upserts.\nUnified streaming and batch data processing allowing us to work with a wide variety of data latencies.\n\nWhen users read a Delta Lake table for the first time or run a new query on an open table, Spark checks the transaction log for new transactions that have been posted to the table. If a change exists, Spark updates the table. This ensures users are working with the most up-to-date information and the user table is synchronized with the master record.\n\n\nPhoton\nThe DBLP arquitecture can pose challenges to the query execution engine for accessing and processing data, so the execution engine has to provide the same performance as a data warehouse, while still having the scalability of a data lake. Photon provides this.\nPhoton is the next generation query engine and\n\nprovides dramatic infrastructure cost savings.\nis compatible with Spark APIs, implementing a more general execution framework for efficient processing of data.\nwe see increased speed for use cases such as data ingestion, ETL, streaming, data science, and interactive queries as well as SQL-based jobs and IoT.\n\nPhoton is compatible with the Apache Spark Dataframes SQL APIs to allow workloads to run without having to make any code changes. It coordinates work and resources transparently, accelerating portions of SQL and Spark queries without tuning or user intervention."
  },
  {
    "objectID": "mlplatforms-databricks-arquitecture.html#unified-governance-and-security",
    "href": "mlplatforms-databricks-arquitecture.html#unified-governance-and-security",
    "title": "Arquitecture",
    "section": "Unified Governance and Security",
    "text": "Unified Governance and Security\nThe more individual access points added to a system such as users, groups or external connectors, the higher the risk of data breaches along any of those lines.\nChallenges to data and AI governance\n\nDiversity of data and AI assets. Data takes many forms beyond files and tables to complex structures such as dashboards, machine learning models, videos or images.\nDisparate and incompatible data platforms. Data warehouses for BI and data lakes for AI, resulting in data duplication and unsynchronized governance models.\nRise of multi-cloud adoption. Each cloud has a unique governance model.\nFragmented tool usage for data governance. It introduces complexity and multiple integration points in the system, leading to poor performance.\n\nDBLP provides solutions for these challenges.\n\nUnity Catalog\nA unified governance solution for all data assets. It provides a common governance model based on ANSI SQL to define and enforce fine-grained access control on all data and AI assets on any cloud. This enables better native performance, management, and security across clouds.\n\nThe common metatada layer for cross-workspace metadata is at the account level. It provides a single access point with a common interface for collaboration from any workspace in the platform, removing data team silos.\nUnity Catalog allows us to restrict access to certain rows and columns to users or groups authorized to query them.\nWith attribute based access control, we can further simplify governance at scale by controlling access to multiple data items at one time.\nUnity Catalog provides a highly detailed audit trail logging who has performed what action against the data\nUnity Calotog has an user interface for data search and discovery, so this way it breaks down data silos and democratized data across the organization.\nThe low latency metadata serving and auto tuning of tables enables Unity Catalog to provide 38 times faster metadata processing compared to Hive metastore.\nAll the transformations and refinements of data is encompassed in data lineage. All of the interactions with data, including where it came from , what other datasets it might have been combined with, who created it and when, what transformations were performed, and other events and attributes are included in a dataset’s data lineage.\n\n\n\nDelta Sharing\nAn open solution to securely share live data to any computing platform.\nBenefits\n\nOpen cross-platform sharing, allowing us to share existing data in Delta Lake and Apache Parquet formats without having to establish new ingestion processes to consume data since it provides native integration with many tools.\nData is shared live without copying it, with data being maintained and governed on the provider’s data lake. This ensures the datasets are reliable in real-time and provide the most current information to the data recipient. Recipients dont have to be on the same cloud or even use the DBLP.\nIt provides centralized administration and governance to the data provider as the data is governed, tracked, and audited from a single location.\nIt is safe and secure with privacy-safe data clean rooms. This means collaboration between data providers and recipients is hosted in a secure environment while safeguarding data privacy.\nIt is a REST protocol that securely shares access to part of a cloud dataset.\n\n\n\nDivided Security Arquitecture\nDBLP provides security by dividing the arquitecture into two separate planes: the control plane and the data plane.\n\nControl Plane\nIt consists of the managed backend services that Databricks provides. This lives in Databricks’ own cloud account and are aligned with whatever cloud service the customer is using. Here Databricks runs the workspace application and manages notebook, logs, configuration, user information and clusters.\n\n\nData Plane\nIt is where the data is processed by clusters of compute resources (known as classic data plane). Unless we choose to use Serverless Compute, the compute resources in the data plane run inside the business owner’s own cloud account.\n\nSecurity of the Data Plane\nSecurity key points: Networking, Servers, Databricks.\n\nFor the networking of the environment, if the business decides to host the data plane, Databricks will configure the networking by default. The serverless data plane networking infrastructure is managed by Databricks in a Databricks cloud service provider account and shared among customers with additional network boundaries between workspaces and clusters.\nFor servers, Databricks’ clusters are run using the lastest hardened system images. Databricks’ clusters are typically short-lived, often terminated after a job, and do not persist data after termination. Code is launched in an unprivileged container to maintain system stability. This security design provides protection against persistent attackers and privilege scalation.\nFor Databricks’ support cases, Databricks’ access to the environment is limited to cloud service provider APIs for automation and support access. Databricks has a custom-built system, allowing their staff access to fix issues or handle support requests, and it requires either a support ticket or an engineering ticket tied expressly to our workspace. Access is limited to a specific group of employees for limited periods of time. Besides, with security audit logs, the initial access event and the support team members actions are tracked.\n\n\n\n\n\nUser Identity and Access\n\nTable ACLs feature uses traditional SQL-based statements to manage access to data and enable fine-grained view-based access.\nIAM instance profiles enable AWS clusters to assume an IAM Role, so users of that cluster automatically access allowed resources without explicit credentials.\nExternal storage can be mounted in access using a securely stored access key.\nSecrets API separates credentials from code when accessing external resources\n\n\n\nData Security\nDatabricks provides encryption, isolation, and auditing throughout the governance and security structure. User can also be isolated at different levels such as\n\nThe workspace level, where each team or department uses a different workspace.\nThe cluster level, where clusters ACLs can restrict users who attach notebooks to a given cluster. For high concurrency clusters, process isolation, JVM white listings and language limitations can be used for safe coexistence of users with different access levels. Single user clusters, if permitted, allow users to create a private dedicated cluster.\n\nDatabricks encryption capabilities are in place both at rest and in motion\n\nFor data-at-rest encryption\n\nControl plane is encrypted.\nData plane supports local encryption.\nCustomers can use encrypted storage buckets.\nCustomers at some tiers can configure customer-managed keys for managed services.\n\nFor data-in-motion encryption\n\nCommunication between control plane and data plane is encrypted.\nOffers optional intra-cluster encryption.\nCustomer code can be written to avoid unencrypted services (e.g., FTP)\n\n\n\n\nCompliance\n\nSOC 2 Type II\nISO 27001\nISO 27017\nISO 27018\nFedRAMP High\nHITRUST\nHiPAA\nPCI\nGDPR and CCPA ready"
  },
  {
    "objectID": "mlplatforms-databricks-arquitecture.html#instant-compute-and-serverless",
    "href": "mlplatforms-databricks-arquitecture.html#instant-compute-and-serverless",
    "title": "Arquitecture",
    "section": "Instant Compute and Serverless",
    "text": "Instant Compute and Serverless\nWith the classic data plane, compute resources are run in the business’s cloud storage account, and clusters perform distributed data analysis using queries in the Databricks SQL workspace or notebooks in the data science and engineering or Databricks machine learning environments.\nChallenges of classic data plane:\n\nCreating clusters is a complicated task. Choosing the correct size, instance type, and configuration for the cluster can be overwhelming.\nEnvironment startup is slow after making the multitude of choices to configure and provision the cluster.\nCostly behaviors, such as leaving clusters running for longer than necessary to avoid startup times and over provisiong of resources to ensure the cluster can handle spikes and data processing needs.\n\n\nServerless Data Plane\nServerless Compute is a fully managed service that Databricks provisions and manages the compute resources for a business in the Databricks Cloud account instead of the business.\n\nThe environment starts immediately.\nIt is elastic so it scales up and down within seconds.\nClusters are available on-demand, and when finished, the resources are release back to Databricks.\nTotal cost of ownership decreases on average between 20-40%.\nAdmin overhead is eliminated.\n\nAt the heart of Serverless Compute is a fleet of database clusters that are always running unassigned to any customer, waiting in a warm state, ready to be assigned within seconds. It has three layers of isolation\n\nThe container hosting the runtime\nThe virtual machine, hosting the container\nVirtual network for the workspace.\n\nEach part is isolated with no sharing or cross-network traffic allowed, ensuring our work is secure."
  },
  {
    "objectID": "mlplatforms-databricks-arquitecture.html#data-management-terminology",
    "href": "mlplatforms-databricks-arquitecture.html#data-management-terminology",
    "title": "Arquitecture",
    "section": "Data Management Terminology",
    "text": "Data Management Terminology\nUnity Catalog provides a common governance model to define and enforce fine-grained access control on all data and AI assets on any cloud. It supports one consistent place for governing all workspaces to discover access and share data, enabling better native performance, management and security across clouds. Key elements of Unity Catalog are metastore, catalogs, schemas, tables, views and functions.\n\nMetastore\nIt is the top-level logical container in Unity Catalog. It’s a construct that represents the metadata. Metadata is the information about the data objects being managed by the metastore and the ACLs governing those lists.\n\n\nStorage Credentials\nCreated by admins and are used to authenticate with cloud storage containers, either external storage, user supplied storage or the managed storage location for the metastore.\n\n\nExternal Location\nIt is used to provide access control at the file level.\n\n\nShare and Recipient\nRelated to Delta Sharing. It is used to explicitly declare shares, read only logical collections of tables. These can be shared with one or more recipients inside or outside the organization. Shares can be used for two main purposes, namely, to secure shared data outside the organization in a performant way, or to provide linkage between metastores and different parts of the world.\n\n\nCatalog\nThe next thing in the addressable data object hierarchy is the catalog. It is the top-most container for data objects in Unity Catalog. A metastore can have as many catalogs as desired, although only those with appropriate permissions can create them.\nThe catalog forms the first part of the three-level namespace that data analysts use to reference data objects in Unity Catalog. Unity Catalog introduces a third level to provide improved data segregation capabilities.\nSELECT * \nFROM catalog.schema.table\n\n\nSchema\nIt acts as a container for data assets like tables and views, and is the second part of the three-level namespace. Catalogs may contain many schemas as desired. A schema is part of traditional SQL and is unchanged by Unity Catalog.\n\n\nTable\nSQL relations consisting of an ordered list of columns. Tables are defined by two distinct elements: the metadata such as comments, tags, list of columns, associated data types, etc; and the data that populates the rows of the table. There are two types of table in the structure: managed table and external table both with the metadata managed by the metastore in the control plane, the difference lies in where the data is stored. In the former, data files are stored in the metastores managed stores location. For the latter, data files is stored in an external storage location.\n\n\nViews\nThey are stored queries executed when you query the view. They perform arbitrary SQL transformations on tables and other views. They are read-only. They do not have the ability to modify the underlying data.\n\n\nUser-defined functions\nIt allows us to encapsulate custom functionality that can be invoke within queries."
  },
  {
    "objectID": "mlplatforms-databricks-workloads-dataengineering.html",
    "href": "mlplatforms-databricks-workloads-dataengineering.html",
    "title": "Data Engineering",
    "section": "",
    "text": "A unified data platform with managed data ingestion, schema detection, enforcement, and evolution, paired with declarative, auto-scaling data flow integrated with a lakehouse native orchestrator that supports all kinds of workflows."
  },
  {
    "objectID": "mlplatforms-databricks.html",
    "href": "mlplatforms-databricks.html",
    "title": "Data Lakehouse",
    "section": "",
    "text": "Here we need to understand what a data lakehouse is as well as its origin and purpose. To do so, we need to study the history of data management and analytics since a data lakehouse solves many challenges of managing big data and improves the performance for many use cases."
  },
  {
    "objectID": "mlplatforms-databricks.html#data-warehouses",
    "href": "mlplatforms-databricks.html#data-warehouses",
    "title": "Data Lakehouse",
    "section": "Data Warehouses",
    "text": "Data Warehouses\nOne of the requisites for business success is the ability to harness data-driven insights for business decisions and innovation. This motivated the creation of data warehouses or systems that could manage and analyze data that was drastically increasing in volume, velocity and variety. In other words, businees needed more than simple relational databases so data warehouses were designed to collect and consolidate this influx of data as well as provide support for BI and analytics.\n\nProperties:\n\nData is structured and cleaned with predefined schemas.\nGovernance and security is given by Access-Control List (ACL) tables.\nReliability, governance and performance.\n\nDisadvantages:\n\nBy construction, semi-structured or unstructured data is not supported.\nIt becomes very expensive when trying to store or analyze any data that don’t fit the schema.\nIt coult take too much time to process data and provide results.\nLimited capabilities to handle data variety and velocity."
  },
  {
    "objectID": "mlplatforms-databricks.html#data-lakes",
    "href": "mlplatforms-databricks.html#data-lakes",
    "title": "Data Lakehouse",
    "section": "Data Lakes",
    "text": "Data Lakes\nThe creation of data lakes arose by the necessity of having a system that could manage the volumes and speeds of multiple data types (structured, semi-structured and unstructured).\n\nProperties:\n\nFlexible data storages.\nMany different sources can be streamed quickly and cheaply in low-cost cloud object stores.\nThe storage dilemma of data warehouses is solved.\n\nDisadvantages\n\nIt is not supportive of transactional data.\nIt can’t enforce data quality, so data reliability of the data stored is questionable since there are several data formats.\nThe performance of analysis is slower due to the large volume of data available.\nData governance is a challenge as well as data privacy enforcement due to the unstructured nature of data."
  },
  {
    "objectID": "mlplatforms-databricks.html#data-lakehouses",
    "href": "mlplatforms-databricks.html#data-lakehouses",
    "title": "Data Lakehouse",
    "section": "Data Lakehouses",
    "text": "Data Lakehouses\nSince data lakes didn’t fully replace data warehouses for reliable BI insights, businesses implemented complex technology stack environments. However, this introduces complexity and delay as data teams are stuck in silos.\n\nChallenges\n\nDisjointed and duplicative data silos.\nData has to be copied between the systems of the stack environment, impacting oversight and data usage governance.\nCost of storing the same information many times.\nDelay actionable insights.\n\n\nThus, businesses needed:\n\na single, flexible, high performance system to support the ever increasing use cases for data exploration, predictive modeling and predictive analytics.\nsystems to support data applications, including SQL analytics, real-time analysis, data science and machine learning.\n\n\nDatabricks Lakehouse Platform\nThe lakehouse platform is an open arquitecture which combines the analytical and control benefits of a data warehouse with the storage and flexible benefits of a data lake to efficiently handle all data types on all clouds for all use cases within a single platform with a one security and governance model for all data assets, resulting in a single reliable source of truth.\n\nProperties\n\nTransaction support, including ACID transactions for concurrent read/write interactions.\nSchema enforcement and governance for data integrity.\nRobust auditing needs.\nData governance to support privacy regulation and data use metrics.\nBI support\nDecoupled storage from compute, each operating on their own clusters, allowing them to scale independently to support specific needs.\nOpen storage formats such as Apache Parquet.\nSupport for diverse data types.\nSupport for diverse workloads.\nEnd-to-end streaming for real-time reports and analyses.\n\n\nDatabricks was founded in 2013 by the creators of Apache Spark, Delta Lake and MLFlow. It was the inventor and pioneer of the data lakehouse arquitecture in 2021.\nThe arquitectural features are realized on the Databricks Lakehouse Platform, including:\n\nReliability and performance of Delta Lake as the Data Lake Foundation.\nFine-grain governance for data and AI with Unity Catalog.\nPersona-based uses cases for all data team members.\nInstant and serverless compute, the compute layer is provided and managed by Databricks on behalf of the customer.\n\nThe benefits of DBLP\n\nCombine the reliability, governance and security of data warehouses with the openness, flexibility and machine learning support of data lakes to provide a single platform that unifies data warehousing and AI use cases.\nEliminates the challenges caused by previous data environments such as data silos, complicated structures, and fractured governance and security structures.\nIt supports many workloads including data warehousing, data engineering, data streaming and data science and machine learning.\nProvides flexibility to use the existing infraestructure and build a modern data stack with unrestricted access to open source data projects and the broad Databricks partner network."
  },
  {
    "objectID": "production-ready-deploy-maintenance.html",
    "href": "production-ready-deploy-maintenance.html",
    "title": "Deployment and Maintenance",
    "section": "",
    "text": "The deployment setting are usually different from the development settings. Therefore, different sets of tools are often involved when moving a FFP to production.\nDeployment may introduce problems that weren’t visible during development such as limited computational resources.\n\nUse additional time to improve user experience.\n\nThe quality of data and model performance needs to be monitored consistently to provide stable services to targeted users."
  },
  {
    "objectID": "production-ready-evaluation.html",
    "href": "production-ready-evaluation.html",
    "title": "Project Evaluation",
    "section": "",
    "text": "The team should revisit the discussions made during project planning to\n\nevaluate whether the project has been carried out successfully or not\n\nDetails of the project need to be recorded to\n\nsuggest and discuss potential improvements so that next projects can be achieved more efficiently."
  },
  {
    "objectID": "production-ready-ffp.html",
    "href": "production-ready-ffp.html",
    "title": "Build Fully Featured Products",
    "section": "",
    "text": "Once the feasibility of the project has been confirmed by the MVP, it must be packaged into an FFP.\n\nPolish up the MVP to build a production-ready deliverable with various optimizations\n\nIntroduce additional data preparation techniques to improve the quality of input data\nSlightly augment the model pipeline for greater model performance if necessary\n\nReimplement the whole pipeline (data preparation and model training) on web services for higher throughput and quality.\n\nAWS, for example."
  },
  {
    "objectID": "production-ready-mvp.html",
    "href": "production-ready-mvp.html",
    "title": "Build Minimum Viable Products",
    "section": "",
    "text": "Assert directions is clear for everyone, then build a MVP.\n\nBuild a simplistic version of the deliverable that showcases the project’s value.\nUnderstand the project’s difficulties and reject paths with greater risks or less promising outcomes\n\nUse the fail fast, fail often philosophy\n\nWork with partially sampled datasets in development settings and ignore insignificant optimizations."
  },
  {
    "objectID": "production-ready-planning.html",
    "href": "production-ready-planning.html",
    "title": "Effective Project Planning",
    "section": "",
    "text": "The project lead must\n\nclearly define what is the purpose of the project (new product, improve existing service, save operational cost)\nclearly define what needs to be achieved by the project\nunderstand groups that can affect or be affected by the project (stakeholders).\n\nThe evaluations metrics need to be defined and agreed upon as they will be revisited during project evaluation.\n\nBusiness-related metrics\nModel-based metrics\nDue dates\nResource usage (available resources)\nTradeoffs can be made between the various metrics. Slight decrease in accuracy may be acceptable if meeting latency requirements is more critical to the project.\n\nTeam members group together to discuss\n\nhow to achieve business objectives using available resources.\nkey members should have a thorough understanding of the available resources that can be allocated to the project.\nindividual responsabilities as well as stakeholders responsabilities\n\nDefine milestones and their associated tasks\n\nA milestone refers to a point in a project where a significant event occurs\nThe ordering of tasks that lead to the goal is called the critical path\nAvailable resources need to be allocated to each task\nset aside a portion of the resources as a backup\n\nEstablish the timeline, an estimate of how long the project would be.\nGenerate a well-documented project playbook, a thorough description of how the project will be carried out, evaluated and defines business objectives. Refer to PMBOK or PRINCE2.\n\nOverview of key deliverables\nList of stakeholders\nGantt chart defining steps and bottlenecks\nDefinition of responsabilities\nTimelines\nEvaluation criteria\n\nOnce the playbook is constructed, every stakeholder must review and revise it until everyone agrees with the contents.\nDefine the process that the team will follow to update other team members and ensure on-time delivery of the project.\n\nSelect a project management methodologies"
  },
  {
    "objectID": "production-ready.html",
    "href": "production-ready.html",
    "title": "Less Theory More Application",
    "section": "",
    "text": "Construct and deploy complex models in machine learning frameworks.\nThe real difficulty with machine learning projects is not only selecting the right algorithm for the given problem but also efficiently preprocessing the necessary data in the right format and providing a stable service.\nThe idea of this “Production-Ready” section is to walk through every step of a machine learning project\nFollowing the Production-Ready book\nEvery machine learning project should begin with planning and understanding the difficulty of the given problem.\nWhat is the scope of the project?\nOnce this question is answered, the next step is to build a MVP.\nIn the context of machine learning, the process of creating an MVP involves: - Preparing a dataset - Exploring various model architectures to come up with a working solution to the given problem\nWhat is machine learning? How does it shapes our daily lives?\nHow are machine learning projects typically carried out? How are they structured?\nProject planning - Comprehension of business objectives - Define appropriate evaluation metrics - Identification of stakeholders - Resource planning - Differences between a minimum viable product (MVP) and a fully featured product (FFP)\nAim: Construct a machine learning project playbook that clearly describes - The goal of the project - Milestones - Tasks - Resource allocation - Timeline"
  },
  {
    "objectID": "production-ready.html#what-is-machine-learning",
    "href": "production-ready.html#what-is-machine-learning",
    "title": "Less Theory More Application",
    "section": "What is machine learning?",
    "text": "What is machine learning?\nIt is a set of methods within the field of artificial intelligence. By extracting meaningful patterns from historical data, the goal of machine learning is to build a model that makes sensible predictions and decisions for newly collected data.\nWe can capture a given pattern with different techniques: deep learning which exploits artificial neural networks, decision trees, bagging or boosting ensembles, linear models, etc.\nOne of the advantages of deep learning over traditional machine learning techniques is that ANNs enable the automatic selection of necessary features whereas traditional ML techniques require features to be manually selected. ANNs are made up of components called neurons. A group of neurons forms a layer and multiple layers are linked together to form a network. We can understood this arquitecture as multiple steps of nested instructions, thus, as the input data passes through the network, each neuron extracts different information and the model is trained to select the most relevant features for the given task. If we consider neurons as the building blocks for pattern recognition then the usual believe is that deeper networks lead to greater performance as they learn the details better.\nThis advantage allows deep learning methods to adapt to a broader range of problems due to its high flexibility. But it is not free, we need large and diverse enough training dataset to train a DL model properly. This leads an increase in training time."
  },
  {
    "objectID": "production-ready.html#overview-of-machine-learning-projects",
    "href": "production-ready.html#overview-of-machine-learning-projects",
    "title": "Less Theory More Application",
    "section": "Overview of Machine Learning Projects",
    "text": "Overview of Machine Learning Projects\nIn general, machine learning projects can be split into the following phases:\n\nProject planning\nBuilding MVPs\nBuilding FFPs\nDeployment and maintenance\nProject evaluation"
  },
  {
    "objectID": "tools-mlops.html",
    "href": "tools-mlops.html",
    "title": "MLOps",
    "section": "",
    "text": "Weights & Biases\nPapermill"
  },
  {
    "objectID": "venv-conda.html",
    "href": "venv-conda.html",
    "title": "Setting up virtual environments with conda",
    "section": "",
    "text": "For more information see Managing environments.\nCreate a virtual environment (replace venvname with the name of environment)\nfoo@bar:~$ conda create -n venvname python=3.9\nVerify that the new environment was installed correctly\nfoo@bar:~$ conda info --envs\nfoo@bar:~$ conda env list\nActivate or deactivate the environment\nfoo@bar:~$ conda activate venvname\nfoo@bar:~$ conda deactivate\nActivate the environment and install required packages\nfoo@bar:~$ pip install -r requirements.txt\nfoo@bar:~$ conda install pytorch torchvision torchaudio cudatoolkit=11.2 -c pytorch -c nvidia\nfoo@bar:~$ conda install --force-reinstall -y -q --name py39 -c conda-forge --file requirements.txt\nSave the environment to a file\nfoo@bar:~$ conda env export > venv.yml\nCreate a environment from a YAML file\nfoo@bar:~$ conda env create -f venv.yml\nDelete the environment\nfoo@bar:~$ conda remove -n venvname --all"
  }
]