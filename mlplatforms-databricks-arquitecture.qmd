---
title: "Arquitecture"
date: last-modified
---

Here we describe the main aspects of the arquitecture behind the Databricks Lakehouse Platform. We start by showing how **Delta Lake** solves many issues related to data reliability and how **Photon** improves the performance of DBLP.

## Data Reliability and Performance
Data needs to be reliable and clean since it's the source for business insights. While data lakes are a great solution for holding large quantities of raw data, they lack important features for data reliability and quality (_data swamps_). Also, data lakes don't often offer as good of performance as that of data warehouses.

Problems with standard data lakes related to data reliability

- Lack of Atomicity, Consistency, Isolation, Durability (ACID) transaction support. It makes it impossible to mix updates, appends and reads.
- Lack of schema enforcement. It creates inconsistent and low quality data.
- Lack of integration with a data catalog. It doesn't create a single source of truth.

Problems with data lakes related to performance

- Data is mostly kept in immutable files.
- Ineffective partitioning. Partitioning tends to be ineffective if the wrong field is selected for partitioning or due to high cardinality columns.
- Too many small files. Appending new data takes the shape of simply adding new files, usually small. It is a known root of query performance degradation.

#### Delta Lake
Delta Lake is a file-based open source storage format that runs on top of existing cloud data lakes. It is compatible with Apache Spark and uses Delta Tables which are based on Apache Parquet. Delta Tables provides versioning, reliability, metadata management, and time travel capabilities making all data types manageable. 

Delta Lake offers

- Guarantees for ACID transactions. No partial or corrupted files.
- Scalable data and metadata handling. Spark scales out all the metadata processing.
- Audit history and time travel. It provides a transaction log with details about every change to data, which allows us to revert to earlier versions for rollback or to reproduce experiments. It makes it possible multi-user work and it creates a single source of truth. 
- Schema enforcement and schema evolution. 
- Support for deletes, updates and merges allowing it to accommodate _Change Data Capture_ (CDC), _Slowly Changing Dimension_ (SCD) operations, and streaming upserts.
- Unified streaming and batch data processing allowing us to work with a wide variety of data latencies.

When users read a Delta Lake table for the first time or run a new query on an open table, Spark checks the transaction log for new transactions that have been posted to the table. If a change exists, Spark updates the table. This ensures users are working with the most up-to-date information and the user table is synchronized with the master record.

#### Photon
The DBLP arquitecture can pose challenges to the query execution engine for accessing and processing data, so the execution engine has to provide the same performance as a data warehouse, while still having the scalability of a data lake. Photon provides this.

Photon is the next generation query engine and 

- provides dramatic infrastructure cost savings.
- is compatible with Spark APIs, implementing a more general execution framework for efficient processing of data.
- we see increased speed for use cases such as data ingestion, ETL, streaming, data science, and interactive queries as well as SQL-based jobs and IoT.

Photon is compatible with the Apache Spark Dataframes SQL APIs to allow workloads to run without having to make any code changes. It coordinates work and resources _transparently_, accelerating _portions_ of SQL and Spark queries without tuning or user intervention.

## Unified Governance and Security

The more individual access points added to a system such as users, groups or external connectors, the higher the risk of data breaches.

Challenges to data and AI governance:

- Diversity of data and AI assets.
- Disparate and incompatible data platforms.
- Rise of multi-cloud adoption
- Fragmented tool usage for data governance. It introduces complexity and multiple integration points in the system.

### How DBLP solves these issues?

#### Unity Catalog
A unified governance solution for all data assets

#### Delta Sharing
An open solution to securely share live data to any computing platform without copying it, with data being maintained on the provider's data lake. This ensures the datasets are reliable in real-time and provide the most current information to the data recipient.

It is a REST protocol that securely shares access to part of a cloud dataset.

#### Control Plane and Data Plane
The arquitecture of DBLP is divided into two separate planes: the control plane and the data plane, to simplify permission, avoid data duplication and reduce risk

##### Control Plane
It consists of the managed backend services that Databricks provides. This lives in Databricks' own cloud account. Here Databricks runs the workspace application and manages notebook, configuration, and clusters

##### Data Plane
It is where the data is processed by clusters of compute resources (known as _classic data plane_). Unless we choose to use Serverless Compute, the compute resources in the data plane run inside the business owner's own cloud account.

Databricks' clusters are typically short-lived, often terminated after a job, and do not persist data after termination. Code is launched in an unprivileged container to maintain system stability.

IAM instance profiles enable AWS clusters to assume an IAM Role, so users of that cluster automatically access allowed resources without explicit credentials


## Instant Compute and Serverless

With the classic data plane, compute resources are run in the business's cloud storage account, and clusters perform distributed data analysis using queries in the Databricks SQL workspace or notebooks in the data science and engineering or Databricks machine learning environments.

Challenges of classic data plane:

- Creating clusters is a complicated task. Choosing the correct size, instance type, and configuration.
- Environment startup is slow after making the multitude of choices for the configuration.
- Long running clusters
- Over provisioning of resources.

Solution is the Databricks Serverless SQL, elastic, secure, instant compute and managed servers.

## Data Management Terminology

### Metastore
It is the top-level logical container in Unity Catalog. It's a construct that represents the metadata. Metadata is the information about the data objects being managed by the metastore.

### Catalog
The next thing in the data object hierarchy is the catalog. It is the top-most container for data objects in Unity Catalog. A metastore can have as many catalogs as desired.

The catalog forms the first part of the three-level namespace that data analysts use to reference data objects in Unity Catalog.

```{sql}
SELECT * 
FROM catalog.schema.table
```

### Schema
It acts as a container for data assets like tables and views, and is the second part of the three-level namespace. Catalogs may contain many schemas as desired.

### Table
SQL relations consisting of an ordered list of columns. Tables have two distinct elements: the metadata such as comments, tags, list of columns, associated data types, etc; and the data that populates the rows of the table. There are two types of table: managed table and external table

### Views
They are stored queries executed when you query the view. They perform arbitrary SQL transformations on tables and other views. They are read-only.

### User-defined functions
Custom functionality that can be invoke within queries.