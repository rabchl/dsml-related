---
title: "Arquitecture"
date: last-modified
---

Here we describe the main aspects of the arquitecture behind the Databricks Lakehouse Platform. We start by showing how **Delta Lake** solves many issues related to data reliability and how **Photon** improves the performance of DBLP. Then, we explain how DBLP solves many challenges of data governance and security with **Unity Catalog**, **Delta Sharing**, and the divided arquitecture into two planes: **Control Plane** and **Data Plane**.

## Data Reliability and Performance
Data needs to be reliable and clean since it's the source for business insights. While data lakes are a great solution for holding large quantities of raw data, they lack important features for data reliability and quality (_data swamps_). Also, data lakes don't often offer as good of performance as that of data warehouses.

Problems with standard data lakes related to data reliability

- Lack of Atomicity, Consistency, Isolation, Durability (ACID) transaction support. It makes it impossible to mix updates, appends and reads.
- Lack of schema enforcement. It creates inconsistent and low quality data.
- Lack of integration with a data catalog. It doesn't create a single source of truth.

Problems with data lakes related to performance

- Data is mostly kept in immutable files.
- Ineffective partitioning. Partitioning tends to be ineffective if the wrong field is selected for partitioning or due to high cardinality columns.
- Too many small files. Appending new data takes the shape of simply adding new files, usually small. It is a known root of query performance degradation.

### Delta Lake
Delta Lake is a file-based open source storage format that runs on top of existing cloud data lakes. It is compatible with Apache Spark and uses Delta Tables which are based on Apache Parquet. Delta Tables provides versioning, reliability, metadata management, and time travel capabilities making all data types manageable. 

Delta Lake offers

- Guarantees for ACID transactions. No partial or corrupted files.
- Scalable data and metadata handling. Spark scales out all the metadata processing.
- Audit history and time travel. It provides a transaction log with details about every change to data, which allows us to revert to earlier versions for rollback or to reproduce experiments. It makes it possible multi-user work and it creates a single source of truth. 
- Schema enforcement and schema evolution. 
- Support for deletes, updates and merges allowing it to accommodate _Change Data Capture_ (CDC), _Slowly Changing Dimension_ (SCD) operations, and streaming upserts.
- Unified streaming and batch data processing allowing us to work with a wide variety of data latencies.

When users read a Delta Lake table for the first time or run a new query on an open table, Spark checks the transaction log for new transactions that have been posted to the table. If a change exists, Spark updates the table. This ensures users are working with the most up-to-date information and the user table is synchronized with the master record.

### Photon
The DBLP arquitecture can pose challenges to the query execution engine for accessing and processing data, so the execution engine has to provide the same performance as a data warehouse, while still having the scalability of a data lake. Photon provides this.

Photon is the next generation query engine and 

- provides dramatic infrastructure cost savings.
- is compatible with Spark APIs, implementing a more general execution framework for efficient processing of data.
- we see increased speed for use cases such as data ingestion, ETL, streaming, data science, and interactive queries as well as SQL-based jobs and IoT.

Photon is compatible with the Apache Spark Dataframes SQL APIs to allow workloads to run without having to make any code changes. It coordinates work and resources _transparently_, accelerating _portions_ of SQL and Spark queries without tuning or user intervention.

## Unified Governance and Security
The more individual access points added to a system such as users, groups or external connectors, the higher the risk of data breaches along any of those lines.

Challenges to data and AI governance

- Diversity of data and AI assets. Data takes many forms beyond files and tables to complex structures such as dashboards, machine learning models, videos or images.
- Disparate and incompatible data platforms. Data warehouses for BI and data lakes for AI, resulting in data duplication and unsynchronized governance models.
- Rise of multi-cloud adoption. Each cloud has a unique governance model.
- Fragmented tool usage for data governance. It introduces complexity and multiple integration points in the system, leading to poor performance.

DBLP provides solutions for these challenges.

### Unity Catalog
A unified governance solution for all data assets. It provides a common governance model based on ANSI SQL to define and enforce fine-grained access control on all data and AI assets on any cloud. This enables better native performance, management, and security across clouds.

- The common metatada layer for cross-workspace metadata is at the account level. It provides a single access point with a common interface for collaboration from any workspace in the platform, removing data team silos.
- Unity Catalog allows us to restrict access to certain rows and columns to users or groups authorized to query them.
- With attribute based access control, we can further simplify governance at scale by controlling access to multiple data items at one time.
- Unity Catalog provides a highly detailed audit trail logging who has performed what action against the data 
- Unity Calotog has an user interface for data search and discovery, so this way it breaks down data silos and democratized data across the organization.
- The low latency metadata serving and auto tuning of tables enables Unity Catalog to provide 38 times faster metadata processing compared to Hive metastore.
- All the transformations and refinements of data is encompassed in **data lineage**. All of the interactions with data, including where it came from , what other datasets it might have been combined with, who created it and when, what transformations were performed, and other events and attributes are included in a dataset's data lineage.

### Delta Sharing
An open solution to securely _share live data_ to any computing platform. 

Benefits

- Open cross-platform sharing, allowing us to share existing data in Delta Lake and Apache Parquet formats without having to establish new ingestion processes to consume data since it provides native integration with many tools.
- Data is shared live without copying it, with data being maintained and governed on the provider's data lake. This ensures the datasets are reliable in real-time and provide the most current information to the data recipient. Recipients dont have to be on the same cloud or even use the DBLP.
- It provides centralized administration and governance to the data provider as the data is governed, tracked, and audited from a single location.
- It is safe and secure with privacy-safe data clean rooms. This means collaboration between data providers and recipients is hosted in a secure environment while safeguarding data privacy.
- It is a REST protocol that securely shares access to part of a cloud dataset.

### Divided Security Arquitecture
DBLP provides security by dividing the arquitecture into two separate planes: the control plane and the data plane.

#### Control Plane
It consists of the managed backend services that Databricks provides. This lives in Databricks' own cloud account and are aligned with whatever cloud service the customer is using. Here Databricks runs the workspace application and manages notebook, logs, configuration, user information and clusters.

#### Data Plane
It is where the data is processed by clusters of compute resources (known as _classic data plane_). Unless we choose to use Serverless Compute, the compute resources in the data plane run inside the business owner's own cloud account. 

##### Security of the Data Plane
Security key points: Networking, Servers, Databricks. 

- **_For the networking of the environment_**, if the business decides to host the data plane, Databricks will configure the networking by default. The serverless data plane networking infrastructure is managed by Databricks in a Databricks cloud service provider account and shared among customers with additional network boundaries between workspaces and clusters.
- **_For servers_**, Databricks' clusters are run using the lastest hardened system images. Databricks' clusters are typically short-lived, often terminated after a job, and do not persist data after termination. Code is launched in an unprivileged container to maintain system stability. This security design provides protection against persistent attackers and privilege scalation.
- **_For Databricks' support cases_**, Databricks' access to the environment is limited to cloud service provider APIs for automation and support access. Databricks has a custom-built system, allowing their staff access to fix issues or handle support requests, and it requires either a support ticket or an engineering ticket tied expressly to our workspace. Access is limited to a specific group of employees for limited periods of time. Besides, with security audit logs, the initial access event and the support team members actions are tracked.

### User Identity and Access

- Table ACLs feature uses traditional SQL-based statements to manage access to data and enable fine-grained view-based access.
- IAM instance profiles enable AWS clusters to assume an IAM Role, so users of that cluster automatically access allowed resources without explicit credentials.
- External storage can be mounted in access using a securely stored access key.
- Secrets API separates credentials from code when accessing external resources

### Data Security
Databricks provides encryption, isolation, and auditing throughout the governance and security structure. User can also be isolated at different levels such as

- The workspace level, where each team or department uses a different workspace.
- The cluster level, where clusters ACLs can restrict users who attach notebooks to a given cluster. For high concurrency clusters, process isolation, JVM white listings and language limitations can be used for safe coexistence of users with different access levels. Single user clusters, if permitted, allow users to create a private dedicated cluster.

Databricks encryption capabilities are in place both at rest and in motion

- For data-at-rest encryption
  - Control plane is encrypted.
  - Data plane supports local encryption.
  - Customers can use encrypted storage buckets.
  - Customers at some tiers can configure customer-managed keys for managed services.

- For data-in-motion encryption
  - Communication between control plane and data plane is encrypted.
  - Offers optional intra-cluster encryption.
  - Customer code can be written to avoid unencrypted services (e.g., FTP)

### Compliance

- SOC 2 Type II
- ISO 27001
- ISO 27017
- ISO 27018
- FedRAMP High
- HITRUST
- HiPAA
- PCI
- GDPR and CCPA ready


## Instant Compute and Serverless

With the classic data plane, compute resources are run in the business's cloud storage account, and clusters perform distributed data analysis using queries in the Databricks SQL workspace or notebooks in the data science and engineering or Databricks machine learning environments.

Challenges of classic data plane:

- Creating clusters is a complicated task. Choosing the correct size, instance type, and configuration.
- Environment startup is slow after making the multitude of choices for the configuration.
- Long running clusters
- Over provisioning of resources.

Solution is the Databricks Serverless SQL, elastic, secure, instant compute and managed servers.

## Data Management Terminology

### Metastore
It is the top-level logical container in Unity Catalog. It's a construct that represents the metadata. Metadata is the information about the data objects being managed by the metastore.

### Catalog
The next thing in the data object hierarchy is the catalog. It is the top-most container for data objects in Unity Catalog. A metastore can have as many catalogs as desired.

The catalog forms the first part of the three-level namespace that data analysts use to reference data objects in Unity Catalog.

```{sql}
SELECT * 
FROM catalog.schema.table
```

### Schema
It acts as a container for data assets like tables and views, and is the second part of the three-level namespace. Catalogs may contain many schemas as desired.

### Table
SQL relations consisting of an ordered list of columns. Tables have two distinct elements: the metadata such as comments, tags, list of columns, associated data types, etc; and the data that populates the rows of the table. There are two types of table: managed table and external table

### Views
They are stored queries executed when you query the view. They perform arbitrary SQL transformations on tables and other views. They are read-only.

### User-defined functions
Custom functionality that can be invoke within queries.