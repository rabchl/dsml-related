---
title: "Arquitecture and Performance"
date: last-modified
---

## Lack of Data Reliability and Performance
While data lakes are a great solution for holding large quantities of raw data, they lack important features for data reliability and quality. Also, data lakes do not offer as good of performance as that of data warehouses.

Problems with data lakes related to data reliability

- Lack of ACID transaction support. It makes it impossible to mix updates, appends and reads.
- Lack of schema enforcement. It creates inconsistent and low quality data.
- Lack of integration with a data catalog. It does not create a single source of truth.

Problems with data lakes related to performance. Data is mostly kept in immutable files leading to

- Ineffective partitioning. Partitioning tends to be ineffective if the wrong field was selected for partitioning or due to high cardenality columns.
- Too many small files. Appending new data takes the shape of simply adding new files, usually small files.

### How DBLP solves these issues?

#### Delta Lake
Delta Lake is a file-based open source storage format. It offers:

- ACID transaction guarantees. No partial or corrupted files.
- Scalable data and metadata handling. Spark scales out all the metadata processing.
- Audit history and time travel. It provides a transaction log with details about every change to data which allows us to revert to earlier versions for rollback or to reproduce experiments.
- Schema enforcement and schema evolution. 
- Unified streaming and batch data processing

It is compatible with Apache Spark, uses Delta Tables which are based on Apache Parquet. Delta Lake has a transaction log which converts it as the single source of truth. It makes it possible multi-user work.

#### Photon
The execution engine has to provide the same performance as a data warehouse, while still having the scalability of a data lake. Photon provides this. It is the next generation query engine.

Photon is compatible with Spark APIs, implementing a more general execution framework for efficient processing of data with support of the Spark APIs.

With Photon we see:

- increased speed for use cases such as data ingestion, ETL, streaming, data science, and interactive queries


## Unified Governance and Security

The more individual access points added to a system such as users, groups or external connectors, the higher the risk of data breaches.

Challenges to data and AI governance:

- Diversity of data and AI assets.
- Disparate and incompatible data platforms.
- Rise of multi-cloud adoption
- Fragmented tool usage for data governance. It introduces complexity and multiple integration points in the system.

### How DBLP solves these issues?

#### Unity Catalog
A unified governance solution for all data assets

#### Delta Sharing
An open solution to securely share live data to any computing platform without copying it, with data being maintained on the provider's data lake. This ensures the datasets are reliable in real-time and provide the most current information to the data recipient.

It is a REST protocol that securely shares access to part of a cloud dataset.

#### Control Plane and Data Plane
The arquitecture of DBLP is divided into two separate planes: the control plane and the data plane, to simplify permission, avoid data duplication and reduce risk

##### Control Plane
It consists of the managed backend services that Databricks provides. This lives in Databricks' own cloud account. Here Databricks runs the workspace application and manages notebook, configuration, and clusters

##### Data Plane
It is where the data is processed by clusters of compute resources (known as _classic data plane_). Unless we choose to use Serverless Compute, the compute resources in the data plane run inside the business owner's own cloud account.

Databricks' clusters are typically short-lived, often terminated after a job, and do not persist data after termination. Code is launched in an unprivileged container to maintain system stability.

IAM instance profiles enable AWS clusters to assume an IAM Role, so users of that cluster automatically access allowed resources without explicit credentials


## Instant Compute and Serverless

With the classic data plane, compute resources are run in the business's cloud storage account, and clusters perform distributed data analysis using queries in the Databricks SQL workspace or notebooks in the data science and engineering or Databricks machine learning environments.

Challenges of classic data plane:

- Creating clusters is a complicated task. Choosing the correct size, instance type, and configuration.
- Environment startup is slow after making the multitude of choices for the configuration.
- Long running clusters
- Over provisioning of resources.

Solution is the Databricks Serverless SQL, elastic, secure, instant compute and managed servers.

## Data Management Terminology